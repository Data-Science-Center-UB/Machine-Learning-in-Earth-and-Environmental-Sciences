{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea51bf7-572f-4a26-be71-2adcd41f20d7",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34818998-85c3-4d37-b44b-f7459449a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn shap PyALE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2703a8-b48c-4631-a3e9-e542a48e2b39",
   "metadata": {},
   "source": [
    "# Post-hoc Model-Agnostic Interpretation Methods\n",
    "\n",
    "Although interpretable or semi-interpretable supervised machine learning (ML) models are available, they are not the most commonly used in geosciences today. This is largely because more complex models such as ensemble methods (e.g. Gradient Boosting, Random Forests) or deep neural networks (NNs) often outperform them in predictive performance (Dramsch et al. 2020). However, complex models require translating complex logic into human-intuitive insights. As a result, much of modern XAI research has shifted toward **post-hoc** interpretation methods that explain the predictions of black-box models after the model has been trained and without simplifying the models themselves (Dwivedi et al. 2023). Since around 2015, this field has expanded rapidly, introducing both model-agnostic methods (e.g., SHAP, LIME, partial dependence plots) and model-specific methods. These methods can potentially not only be used to justify model predictions, but to enhance process understanding in the geosciences (Jiang et al. 2024; see Notebook 1).\n",
    "\n",
    "**Model-agnostic** interpretation methods aim to explain ML models without relying on their internal structures such as weights or coefficients. They follow the **SIPA** principle: \n",
    "- **S**ample from the data,\n",
    "- **I**ntervene on it,\n",
    "- make **P**redictions using the model,\n",
    "- and then **A**ggregate the results.\n",
    "\n",
    "A key benefit of model-agnostic interpretation is the flexibility it offers. The same interpretation method can be used across different models and switching methods without retraining the model is easy.\n",
    "\n",
    "Model-agnostic methods can be classified into **local** and **global** approaches. Local methods explain individual predictions. With that, they can also be used to find anomalies or explain why specific predictions are wrong. Global model-agnostic methods, on the other hand, describe a model's overall behavior across the dataset. Two broad categories within global model-agnostic methods are **feature effects** and **feature importance**. In general, the usefulness of model-agnostic interpretation for both local and global methods depends on model performance (Molnar 2025). \n",
    "\n",
    "This notebook introduces three local and three global model-agnostic interpretation methods, which are briefly explained below. For more details and additional methods, see for example Molnar (2025).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../Images/XAI_Model_Agnostic.png\" style=\"width: 400px;\">\n",
    "  <div style=\"font-size: 14px; margin-top: 8px;\">Fig. 1 Overview of interpretability methods in machine learning, modified from Molnar (2025)</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d496968-f206-445b-b19d-10ede7b9f46e",
   "metadata": {},
   "source": [
    "## 1. Local Model-Agnostic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b355a-a2fc-44a5-99eb-c350622953db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 Individual Conditional Expectation (ICE)\n",
    "\n",
    "Individual Conditional Expectation (ICE) (Goldstein et al. 2015) plots show how the prediction for a single data instance changes when a specific feature is varied, while all other features are kept constant. Each line in an ICE plot represents one instance, with predictions generated by altering the chosen feature across a range of values. This allows us to visualize how sensitive the model’s predictions are to that feature on an individual level.\n",
    "\n",
    "The method is simple and intuitive, revealing individual prediction patterns and heterogeneous feature effects. However, it is limited to one feature at a time and may produce unrealistic samples when features are correlated, since ICE varies one feature while holding others constant, potentially violating feature dependencies in the data. It often makes sense to explore the plot using transparency when lines are overlapping or by coloring another feature that reveals interaction. If plots become too crowded, sampling or using a partial dependence plot (PDP; see Sect. 2.1) may help. In addition, centered ICE plots (c-ICE) align all curves to the prediction at a chosen anchor point (e.g., the minimum value), showing only relative changes and derivative ICE plots (d-ICE) visualize the rate of change of predictions with respect to the feature. If the derivatives vary across instances, it indicates interactions. \n",
    "\n",
    "See Section 2.1 for an example usage of ICE plots together with PDP plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c922ca-dd7f-4821-94fe-fdcd48d7d84e",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 1.2 Local Interpretable Model-Agnostic Explanations (LIME)\n",
    "\n",
    "LIME (Ribeiro, Singh, and Guestrin 2016) explains individual predictions of black box models by training a simple, understandable model (like linear regression or a decision tree) to mimic the black box model's behavior near a specific data point. It does this by creating slightly modified versions of that data point, asking the black box model for predictions on each one, weighting them based on how similar they are to the original, and then fitting the simple model to these weighted examples (e.g. linear regression or a decision tree). The learned model should be a good approximation of the black box model predictions locally, but it does not have to be a good global approximation. LIME is able to explain any black box and can also be applied to text or image data.\n",
    "\n",
    "LIME has the following steps:\n",
    "1. **Select the instance** of interest we want to explain the prediction for.\n",
    "2. **Generate samples** by permuting feature values that are sampled from a **normal distribution**.\n",
    "3. **Assign a weight** to each sample based on how far they are to the individual instance. LIME gives more importance to data points that are closer to the instance being explained. To do this, it uses a function called an exponential kernel, which assigns higher weights to nearby points and lower weights to distant ones. The size of the neighborhood - how far points can be while still influencing the explanation - is controlled by a parameter called kernel width. \n",
    "4. **Make predictions on permutations** using the original black box model.\n",
    "5. **Train a surrogate model** using the weighted samples & predictions.\n",
    "6. **Interpret** the surrogate model.\n",
    "\n",
    "Current limitations with LIME were summarized by Molnar 2025: \n",
    "- Because data points are sampled from a Gaussian distribution, the correlation between features is ignored. This can lead to unlikely data points, which are then used to learn the local explanation models.\n",
    "- Changing the kernel width can significantly affect the explanation. In high-dimensional data, defining what \"close\" means becomes even harder, since not all features are equally scaled or meaningful in the same way. The choice of neighborhood is an unsolved problem and different kernel widths should be tested.\n",
    "- LIME explanations can be unstable: small changes in the input or repeating the process can lead to very different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dff06d-97e4-43c5-b1bc-d0a92618a67b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 1.3 Shapley Values and Shapely Additive Explanations (SHAP)\n",
    "\n",
    "**Shapley values** come from cooperative **game theory**, introduced by Lloyd Shapley (1953), to fairly distribute a payoff among players based on their contribution. In ML, the prediction for a specific instance is the “payout”, and the features are the players. The goal is to attribute the difference between the prediction and the average prediction to individual features.\n",
    "\n",
    "Step-by-step for computing the shapley value of one feature:\n",
    "1. Form all possible **subsets** (coalitions) of the other features. The number of subsets grows exponentially with the number of features. That’s why we use Monte Carlo sampling to avoid having to evaluate every possible subset, which becomes computationally impossible as the number of features increases.\n",
    "2. For each sampled subset:\n",
    "    - Compute **prediction with the target feature**.\n",
    "    - Compute **prediction without the target feature**.\n",
    "    - The difference is the **marginal contribution** of the feature.\n",
    "4. Average the marginal contributions, weighted appropriately by subset size. The result is the shapley value for the feature.\n",
    "\n",
    "The Shapley value of a feature quantifies how much that feature contributed to the difference between the actual prediction and the average prediction for the dataset. **SHAP** (SHapley Additive exPlanations), introduced by Lundberg & Lee (2017), is a practical framework for computing and visualizing Shapley values. It bridges shapley values and LIME. There are multiple ways to estimate shapley values for different algorithms such as KernelSHAP, Permutation Method, and TreeSHAP (for tree-based methods). The Python `shap` package includes a wide range of visualization and aggregation tools (some displayed below). These can provide information about feature importances and effects (including interaction effects). Compared to SHAP, LIME approximates these effects using a local surrogate model, which may not fully capture the original model's behavior.\n",
    "\n",
    "As with other model-agnostic methods, Shapley values don’t provide causality. They show contributions, not what-if (counterfactual) outcomes. They’re also computationally expensive, as exact calculation requires evaluating all feature subsets (so we rely on approximation via sampling). Another limitation is that SHAP assumes that features are independent unless you're using a version of SHAP that explicitly models conditional distributions. For example, see [`shap.TreeExplainer` documentation](https://shap.readthedocs.io/en/latest/generated/shap.TreeExplainer.html) where the `feature_perturbation` parameter controls this behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666b637-f4a8-4ab8-ba9e-04438a25b3ce",
   "metadata": {},
   "source": [
    "---\n",
    "In the following example, we build on the framework presented by Flora et al. (2024), who applied various XAI methods across multiple atmospheric science tasks. Our focus is on their road surface temperature prediction use case, which involves a binary classification task: predicting whether the road surface temperature is below freezing (0°C) based on near-surface meteorological conditions. The dataset contains approximately 1 million examples collected over two cool seasons and includes 30 input features from temperature variables (e.g., surface and 2-m air temperature), radiation and heat fluxes, cloud coverage, freezing duration metrics, and site-level features (e.g., urban/rural classification and wind speed). The model used is a Random Forest classifier, trained to distinguish freezing from non-freezing road conditions.\n",
    "\n",
    "For demonstration purposes, we load a much smaller random sample of the original dataset to keep the notebook lightweight. After training on this dataset, we then apply SHAP to interpret the model's predictions, following the structure and insights from the original study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f5a7d-7001-44c0-a457-cc6d8fc79dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914be06e-c7ad-4563-8e6a-8f2f5dfb78e0",
   "metadata": {},
   "source": [
    "Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c0126-a64b-4309-b2df-1a64eb3d7d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Flora_et_al_2024/road_surface_dataset_sampled.csv\") \n",
    "df.dropna(inplace=True)\n",
    "df.head()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04438621-a5fb-4710-93e3-25ed4456255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from dataset column paper name (inferred from Table 3 in Flora et al. 2024; mappings are approximate and not guaranteed to be fully accurate)\n",
    "column_mapping = {\n",
    "    'sfc_temp': 'Tsfc', 'temp2m': 'T2m', 'dwpt2m': 'Td', 'hrrr_dT': 'HRRRdT',\n",
    "    'swave_flux': 'S', 'vbd_flux': 'Vbd', 'vdd_flux': 'Vdd', 'uplwav_flux': 'λ↑',\n",
    "    'dllwave_flux': 'λ↓', 'sat_irbt': 'Tirbt', 'lat_hf': 'Lhf', 'sens_hf': 'Shf',\n",
    "    'gflux': 'G', 'd_ground': 'DG²', 'd_rad_d': 'DS² ↓', 'd_rad_u': 'DS² ↑',\n",
    "    'tot_cloud': 'Ctotal', 'low_cloud': 'Clow', 'mid_cloud': 'Cmid', 'high_cloud': 'Chigh',\n",
    "    'tmp2m_hrs_bl_frez': 'Hours T2m ≤ 0°C', 'tmp2m_hrs_ab_frez': 'Hours T2m ≥ 0°C',\n",
    "    'sfcT_hrs_bl_frez': 'Hours Tsfc ≤ 0°C', 'sfcT_hrs_ab_frez': 'Hours Tsfc ≥ 0°C',\n",
    "    'fric_vel': 'Vfric', 'sfc_rough': 'SR', 'wind10m': 'U10m',\n",
    "    'urban': 'Urban', 'rural': 'Rural', 'date_marker': 'Date Marker'\n",
    "}\n",
    "\n",
    "# Reduce and rename\n",
    "df_reduced = df[list(column_mapping)].rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308cc57-8e62-4bdd-b124-7ca52374ee59",
   "metadata": {},
   "source": [
    "Define y and X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed39e627-b7a7-4208-8c73-e3931419ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['cat_rt']\n",
    "X = df_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc3fab-d0d3-4270-8d7b-9d95f6d05d7f",
   "metadata": {},
   "source": [
    "Apply train-test-split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d892f-6d17-4dc6-bdfd-e2dd28dfbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d35f31-7e01-4c28-be3d-1a57316439c2",
   "metadata": {},
   "source": [
    "Train and evaluate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2582900-a956-43b7-9c82-b2eefbd2ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445bb764-3289-4881-a71f-bb3205b7c02d",
   "metadata": {},
   "source": [
    "SHAP scales poorly with large datasets. For this demonstration we therefore calculate SHAP values faster using approximation. `approximate=True` tells SHAP to speed up calculations by simplyfying calculations (not the full path-dependent decomposition is applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ecc8ee-6761-4449-a7d8-6b3e011aa10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP TreeExplainer\n",
    "explainer = shap.TreeExplainer(\n",
    "    model,\n",
    "    data=X_test,\n",
    "    feature_perturbation='interventional'  # safe default\n",
    ")\n",
    "\n",
    "# Compute SHAP values with approximation (faster)\n",
    "shap_values = explainer.shap_values(X_test, approximate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6c8fa-f2f5-441e-b142-201a4a32ada0",
   "metadata": {},
   "source": [
    "Next, we generate several SHAP plots inspired by Flora et al. (2024). Please note that, in contrast to the original study, we work with a substantially smaller dataset and use SHAP’s approximation mode to speed up computation, which lead to some differences in the resulting explanations.\n",
    "\n",
    "**1) SHAP summary plot**\n",
    "\n",
    "A SHAP summary plot aggregates SHAP values for all samples and all features to show overall feature importance and effect distributions. Each dot represents the SHAP value of one feature for one sample. \n",
    "- Features are ordered top to bottom by overall importance (mean absolute SHAP value).\n",
    "- The x-axis shows the SHAP value (impact on model output):\n",
    "    - Negative values push the prediction lower (less freezing).\n",
    "    - Positive values push the prediction higher (more freezing).\n",
    "- The color shows the actual feature value for that sample:\n",
    "    - Red = high feature value\n",
    "    - Blue = low feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a535eb55-22f8-4e02-b32b-847223a57b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select SHAP values for class 1 (positive class)\n",
    "shap_values_class1 = shap_values[:, :, 1]  # shape: (14257, 29)\n",
    "\n",
    "# Plot summary plot\n",
    "shap.summary_plot(shap_values_class1, \n",
    "                  X_test,\n",
    "                  plot_size=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4d263-c370-4f45-b38a-9138bc30a115",
   "metadata": {},
   "source": [
    "- Tsfc (surface temperature) is the most important. This is consistent with physical intuition (Flora et al. 2024). Cold surface temperatures (blue dots) have positive SHAP values, increasing freezing likelihood. Warm temperatures (red dots) have negative SHAP values, decreasing freezing probability.\n",
    "- Also freezing duration features are important. Compared to that, most radiation and cloud features have smaller but non-negligible effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5282851-bd27-41f0-bb3e-da8d86de9104",
   "metadata": {},
   "source": [
    "**2) SHAP dependence plot**\n",
    "\n",
    "A SHAP dependence plot shows:\n",
    "- The relationship between a single feature's value (code below: λ↑ on the x-axis) and its SHAP value (y-axis). The SHAP value measures how much that feature contributes to pushing the model output up or down for each sample.\n",
    "- Points are colored by the value of a second interacting feature (code below: Tsfc), which helps visualize interaction effects.\n",
    "    - Blue = colder surface temperatures.\n",
    "    - Red = warmer surface temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e28d8e-0bcf-449b-b1e1-528936ec0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    ind='λ↑',\n",
    "    shap_values=shap_values_class1,\n",
    "    features=X_test,\n",
    "    interaction_index='Tsfc',\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d279c88-dc4e-4f15-b6f7-9c8942ce4064",
   "metadata": {},
   "source": [
    "We see:\n",
    "- When λ↑ (upward longwave radiation) is low (~200-280):\n",
    "    - SHAP values are mostly positive, so λ↑ increases the freezing probability.\n",
    "    - Points with blue colors (colder Tsfc - surface temperature) tend to have higher SHAP values.\n",
    "- When λ↑ rises above ~300:\n",
    "    - SHAP values become mostly negative, meaning high λ↑ actually reduces freezing probability.\n",
    "    - This effect varies with Tsfc — red points (warmer surfaces) show a stronger negative impact.\n",
    "In summary, there is a nonlinear relationship and interaction between λ↑ and Tsfc: For colder surfaces, moderate λ↑ values help predict freezing. For warmer surfaces, high λ↑ decreases freezing prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc191be-600d-4ddf-85d5-7d625a4bc01d",
   "metadata": {},
   "source": [
    "**3) SHAP waterfall plot**\n",
    "\n",
    "With the SHAP waterfall plot, we are testing how the model arrives at its prediction for one specific road surface instance. Let's use the plot to illustrate which features most influenced the model’s uncertain decision, specifically the instance whose predicted probability of frozen road (class 1) is closest to 0.5 (the decision threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ee2f0-6f4c-4cfb-be2e-a733fecedbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities for positive class\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Find sample closest to decision threshold (0.5)\n",
    "index = np.argmin(np.abs(probs - 0.5))\n",
    "print(f\"Chosen index: {index}, prediction: {probs[index]:.3f}\")\n",
    "\n",
    "# Get instance\n",
    "instance = X_test.iloc[[index]]\n",
    "\n",
    "# Explain prediction\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(instance)\n",
    "\n",
    "# Create explanation for class 1\n",
    "explanation = shap.Explanation(\n",
    "    values=shap_values.values[0][:, 1],\n",
    "    base_values=shap_values.base_values[0][1],\n",
    "    data=shap_values.data[0],\n",
    "    feature_names=shap_values.feature_names\n",
    ")\n",
    "\n",
    "# Plot waterfall\n",
    "shap.plots.waterfall(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d65b14-8428-4410-b984-6c5d4c71c4f9",
   "metadata": {},
   "source": [
    "What does the plot show?\n",
    "- Base value: The average model output over the whole dataset. For example, E[f(x)] = 0.396 means on average the model predicts a 39.6% chance of frozen road.\n",
    "- Final prediction: The model’s prediction for this specific instance (f(x) = 0.5), i.e., 50% frozen probability.\n",
    "- Colored bars: Each bar corresponds to a feature’s contribution (SHAP value) to move the prediction from the base value toward the final prediction.\n",
    "    - Red bars (+) increase the predicted probability of freezing.\n",
    "    - Blue bars (−) decrease the predicted probability of freezing.\n",
    "- Feature values on the left: The actual value of the feature for this instance (e.g., T2m = 1.651), sorted by importance.\n",
    "\n",
    "In summary, the final prediction of 0.5 comes from a balance of (temperature) features pushing both ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68048b2-f309-4bcd-adc3-4688b565b160",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 2. Global Model-Agnostic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add22a27-6d93-4dfb-b035-e9db9996993a",
   "metadata": {},
   "source": [
    "## 2.1 Partial Dependence Plot (PDP)\n",
    "\n",
    "While ICE plots (see Section 1.1) provide individual-level insights into how a prediction changes when one feature is varied, Partial Dependence Plots (PDPs) (Friedman 2001) offer a global perspective by averaging these effects over the entire dataset. A PDP shows the average predicted outcome for different values of a selected feature (or pair of features), marginalizing over all other features. This allows us to understand the general trend of how the model responds to a feature, without focusing on any single data point. The averaging process smooths out individual variations and provides a clearer picture of overall model behavior. However, like ICE plots, PDPs can be misleading when features are strongly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97a3d9-c926-427d-89d8-978d1bcc36b4",
   "metadata": {},
   "source": [
    "---\n",
    "We now use PDP and ICE plots to interpret a Random Forest regressor trained on a bike sharing demand dataset to predict the number of bike rentals using weather conditions, season, and time-related features. This example is adapted from Molnar (2025) and [scikit-learn.org](https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f9fe8-739e-40f8-8b91-79c03ee8ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.inspection import PartialDependenceDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b4e239-381f-4ddd-a7d5-519445bb043d",
   "metadata": {},
   "source": [
    "Load and preprocess the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa540a9-75f8-4fb3-9f7e-20bacf7785f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bikes = fetch_openml(\"Bike_Sharing_Demand\", version=2, as_frame=True)\n",
    "print(bikes)\n",
    "\n",
    "X, y = bikes.data.copy(), bikes.target\n",
    "\n",
    "# Downsample for faster computation\n",
    "X, y = X.iloc[::5], y[::5]\n",
    "\n",
    "# Simplify rare category in \"weather\"\n",
    "X[\"weather\"] = X[\"weather\"].replace(\"heavy_rain\", \"rain\").astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62993f-1fe5-4f2b-bc26-a68b29ecfdb2",
   "metadata": {},
   "source": [
    "Split into train and test based on year (train on year 0 and test on year 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f122d-24ee-4f78-a2f1-977663d67c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = X[\"year\"] == 0.0\n",
    "X = X.drop(columns=\"year\")\n",
    "X_train, y_train = X[mask_train], y[mask_train]\n",
    "X_test, y_test = X[~mask_train], y[~mask_train]\n",
    "\n",
    "# Identify feature types\n",
    "numerical_features = [\"temp\", \"feel_temp\", \"humidity\", \"windspeed\"]\n",
    "categorical_features = X_train.columns.difference(numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1546c7-277a-49c8-ac02-244c1274712e",
   "metadata": {},
   "source": [
    "We inspect the dataset to determine data types and apply appropriate preprocessing. We then apply preprocessing: numerical features are passed through without changes (\"passthrough\"), while categorical features are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33639997-4f09-4f46-9b41-787c5a03fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", \"passthrough\", numerical_features),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17b628-0267-416a-b383-287d7ba013a1",
   "metadata": {},
   "source": [
    "Define Random Forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b9cab-58fb-4e26-af09-a027caca3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestRegressor(n_estimators=100, random_state=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1135e3d-1c62-44d5-96bf-944d87e55459",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95620200-7d13-4c1d-94fb-628ca4d272cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Test R^2 Score: {rf_model.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f83c5e-9295-4f4b-a304-4808352739a5",
   "metadata": {},
   "source": [
    "Plot PDP and ICE for multiple features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7adb990-a5d4-4fb6-9e05-82e141d73657",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(10, 3), sharey=True, constrained_layout=True)\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    rf_model,\n",
    "    X_train,\n",
    "    features=[\"temp\", \"humidity\", \"windspeed\"],\n",
    "    kind=\"both\",\n",
    "    centered=False,\n",
    "    subsample=100,\n",
    "    grid_resolution=20,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "for axis in ax:\n",
    "    legend = axis.get_legend()\n",
    "    legend.remove()\n",
    "    axis.set_ylabel(\"Predicted bike rentals\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8aca43-5fca-4667-a591-deb8fcac9d0e",
   "metadata": {},
   "source": [
    "- The x-axes in our plots include black tick marks at the bottom. These are rug plots and they show the distribution of the training data along each feature. The denser the ticks in a region, the more data the model saw there. Here, windspeed shows a cluster of samples at lower values (around 8), while temperature and humidity are more evenly distributed across their ranges.\n",
    "- The y-axis shows the predicted outcome (number of bike rentals). We see that some ICE curves remain flat while the slope of others increases sharply beyond certain thresholds (e.g., 10°C or 80% humidity), indicating variation in individual predictions.\n",
    "- The dashed line shows the average effects (PDPs). The combination of ICE and PDP gives both the average trend and the individual-level nuances captured by the Random Forest model. Overall, temperature affects bike rentals positively, and humidity negatively, while windspeed appears to have minimal impact because of rather flat curves (changing windspeed does not significantly alter predictions).\n",
    "- While most ICE curves follow a similar pattern, the noticeable divergence in some temperature and humidity curves suggests potential interactions with other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e466b42-7bb4-4cfe-bea4-5e7e0efe1c23",
   "metadata": {},
   "source": [
    "--\n",
    "\n",
    "Next, let's plot the ICE curves colored by season to explore potential interactions between humidity and seasonal patterns. \n",
    "\n",
    "To customize how ICE curves are plotted we have to compute the ICE curves ourselves (again for a subset of the data). For that, we first define a range of humidity values we’ll use to vary the feature. Inside the loop, we:\n",
    "1. Fix one instance (row).\n",
    "2. Change only humidity through the grid of values.\n",
    "3. Keep all other features constant.\n",
    "4. Predict each time, collect results = ICE curve.\n",
    "5. Plot the ICE line, colored by season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12099e4-aa21-46e5-a49c-8d140524aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset 100 random samples for ICE\n",
    "subset = X_train.sample(n=100, random_state=0)\n",
    "seasons = subset[\"season\"].values\n",
    "\n",
    "# Define a range of humidity values\n",
    "humidity_range = np.linspace(X_train[\"humidity\"].min(), X_train[\"humidity\"].max(), 20)\n",
    "\n",
    "# Define color palette for seasons\n",
    "season_colors = {\n",
    "    \"WINTER\": \"#6b6bd6\",  \n",
    "    \"SPRING\": \"#a3b8e0\", \n",
    "    \"SUMMER\": \"#5ecb89\", \n",
    "    \"FALL\":   \"#f2c04c\" \n",
    "}\n",
    "\n",
    "# Compute and plot ICE curves\n",
    "fig, ax = plt.subplots(figsize=(5, 3), constrained_layout=True)\n",
    "\n",
    "for _, row in subset.iterrows():\n",
    "    season = row[\"season\"].upper() \n",
    "    preds = []\n",
    "    for h in humidity_range:\n",
    "        row_mod = row.copy()\n",
    "        row_mod[\"humidity\"] = h\n",
    "        preds.append(rf_model.predict(pd.DataFrame([row_mod]))[0])\n",
    "    ax.plot(humidity_range, preds, color=season_colors[season], alpha=0.7)\n",
    "ax.set_ylabel(\"Predicted bike rentals\")\n",
    "\n",
    "# Add custom legend\n",
    "legend_handles = [\n",
    "    Line2D([0], [0], color=color, lw=2, label=season.title())\n",
    "    for season, color in season_colors.items()\n",
    "]\n",
    "ax.legend(handles=legend_handles, title=\"Season\", loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c71d8-ed76-42a1-ae30-30dab1c825f0",
   "metadata": {},
   "source": [
    "- Coloring ICE curves by season reveals that different seasons have different baseline predictions (intercepts), with winter generally lower and summer higher.\n",
    "- The effect of humidity varies slightly by season: in winter, humidity has less impact than in other months where bike rentals drop more sharply above 60% humidity. \n",
    "\n",
    "--\n",
    "\n",
    "Plotting PDPs with two features of interest enable us to visualize interactions among them. Let's do that for the features \"temp\" and \"humidity\" and their interaction. The third subplot (rightmost) visualizes the model’s average predictions while systematically varying both features together as a heatmap with contours (lines of constant predicted values). These reveale whether their influence on bike rentals is additive (no interaction) or non-additive (interaction present). `PartialDependenceDisplay` automatically understands that the tuple `(\"temp\", \"humidity\")` implies an interaction term and generates the appropriate 2D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea465d19-0662-4b90-b966-d6e56a79296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(10, 3), constrained_layout=True)\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    rf_model,\n",
    "    X_train,\n",
    "    features=[\"temp\", \"humidity\", (\"temp\", \"humidity\")],\n",
    "    kind=\"average\",\n",
    "    subsample=50,\n",
    "    grid_resolution=20,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "for axis in ax:\n",
    "    axis.set_ylabel(\"Predicted bike rentals\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc46ea-03d4-485b-9e31-88f27ae61d8e",
   "metadata": {},
   "source": [
    "> ### **Exercise 1:**\n",
    "> Examine the 2D PDP plot of temperature and humidity (rightmost plot). At what temperature range does the relationship between these features and predicted bike rentals change noticeably, and what does this tell you about how the model uses interactions between features to make predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e508a3e-f9a4-4999-be8b-60000efafe96",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 2.2 Accumulated Local Effects (ALE)\n",
    "\n",
    "Accumulated Local Effects (ALE) (Apley and Zhu 2020) plots help explain how a single feature affects a model’s predictions on average, and are designed to be a more reliable alternative to PDPs when features are correlated. Like PDPs, they aim to describe the model’s behavior, but they work very differently under the hood. Remember that PDPs show the effect of one feature by replacing it with fixed grid values across all observations while keeping all other features unchanged. The model then predicts for these altered instances, and the predictions are averaged. However, if the replaced feature is correlated with others, this process creates unrealistic or impossible combinations (e.g. a large house with only one room). These implausible combinations can bias the result, making PDPs unreliable in the presence of correlated features.\n",
    "\n",
    "ALE avoids this problem by only considering prediction changes for real data points. Instead of replacing feature values globally, **ALE looks at small intervals of the feature and measures how predictions change within these narrow windows**. The key idea is to take the difference in predictions when a feature increases slightly (e.g., from 10°C to 12°C), but only for observations where this change actually makes sense given the data. These local differences are averaged per interval and accumulated across the feature range. In contrast to PDP, ALE never alters the other features and works entirely with values that appear in the data.\n",
    "\n",
    "In addition to showing the effect of a single feature (1D ALE), ALE can also reveal how two features interact (2D ALE). These second-order ALE plots isolate the interaction effect, removing the influence of the individual features. For instance, both high temperature and high humidity may separately reduce bike rentals in our coding example, but their combination might reduce them even more or less than expected from the individual effects. ALE shows this directly, whereas PDP mixes main and interaction effects.\n",
    "\n",
    "Limitations of ALE include that the reflect average effects and do not provide individual-level curves like ICE plots. Interpretation is within intervals, not across the entire range. And if features are very strongly correlated, ALE may still struggle to isolate the effect of one feature from another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a123f79-c64a-426f-a62d-19a448e97715",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following code uses the `PyALE` library to compute and visualize a 2D ALE plot for the bike rental prediction model. Specifically, it investigates the interaction between temperature and humidity, i.e. the same two features we explored before using PDPs.\n",
    "\n",
    "The `grid_size` parameter controls how finely the feature space is divided. A larger grid size gives a more detailed view of the interaction but may become noisy if data is sparse. With `grid_size=15`, both features are split into 15 intervals, creating a 15×15 grid (225 regions). For ~1700 rows, this means each region has enough data to estimate effects reliably. It balances detail and stability: fine enough to show interactions without becoming too noisy or too smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671be6c-db33-490a-8a9c-77dbf22deff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyALE import ale\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Define feature names\n",
    "features = X_train.columns.tolist()\n",
    "\n",
    "# 2. Generate 2D ALE for \"temp\" and \"humidity\"\n",
    "ale_eff_2d = ale(\n",
    "    X=X_train,                         # DataFrame with original feature names\n",
    "    model=rf_model,                   # Your pipeline model\n",
    "    feature=[\"temp\",\"humidity\"],    # 2D interaction\n",
    "    grid_size=15                      # Increase for finer resolution\n",
    ")\n",
    "\n",
    "# 3. The function automatically plots the 2D ALE!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc368be8-a695-40f1-9426-3a57a155dc3d",
   "metadata": {},
   "source": [
    "The plot shows how the combined influence of the two features deviates from what we would expect if their effects were simply additive. This means that the color scale represents only the second-order interaction effect and not the overall contribution of temperature or humidity alone. Yellow to lightgreen shade indicates an above average and darker violet shade a below average prediction when the main effects are already taken into account (Molar 2025).\n",
    "\n",
    "- Overall, this plot confirms that the model detects nonlinear interactions: the impact of temperature on bike rentals depends on humidity and vice versa. If there were no interaction, the plot would display smooth, uniform color transitions along rows or columns, indicating that the combined effect is simply the sum of two independent (additive) effects. \n",
    "- However, the plot highlights non-additive effects, for example: cold and humid weather leads to a higher prediction than expected. While the PDPs showed that both low temperature and high humidity individually reduce bike rentals, the 2D ALE plot additionally reveals that their combined effect is not simply the sum of the two. Instead, in cold and humid conditions, the model predicts more rentals than the main effects alone would suggest, indicating a true interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6278ff-b6e9-4cd6-9804-69a4dd76e659",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 2.3 Permutation Feature Importance (PFI)\n",
    "\n",
    "Feature importance tells us which input features matter most for the model's predictions. But how we calculate this importance really matters. In Notebook 3, we first saw the default feature importance from Random Forest. This is based on how much each feature helps to reduce uncertainty (impurity) in decision trees. It's fast but biased. It favors numerical features over categorical ones and features with many unique values (even if they are random noise). In addition, feature importance is inherently calculated during training, not during testing. \n",
    "\n",
    "To get a more reliable measure of feature usefulness, we can use Permutation Feature Importance (PFI). PFI takes a different approach: instead of looking at how the model was trained, it tests how the model behaves after training, typically using test data. It works by shuffling (permuting) the values of each feature and observing how much the model's performance drops. The idea is:\n",
    "1. Measure how well the model performs normally (e.g., using accuracy).\n",
    "2. Shuffle one feature’s values to break its connection to the target.\n",
    "3. Re-measure performance.\n",
    "- If performance drops a lot, the feature was important.\n",
    "- If there's little or no change, the feature wasn’t contributing much.\n",
    "\n",
    "However, standard PFI shuffles features randomly, which breaks all relationships the feature had with other features (similar to PDPs). This can result in unrealistic or impossible data combinations. To solve this, Conditional PFI shuffles more carefully — for example, by only shuffling within logical subgroups (like species, regions, or categories). This helps maintain realistic feature interactions while still measuring the feature’s unique contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003eca6b-ea3f-4022-be1a-fe15ce779470",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's now extend our workflow on the penguins dataset from Notebook 3 by adding PFI, which will help us evaluate how much each feature actually contributes to the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2660f9a-9c85-44ab-95d6-931c4f4dac89",
   "metadata": {},
   "source": [
    "First, run the model as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e1ac5-3e20-4787-8b60-a4c95805fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load and clean the dataset\n",
    "penguins = sns.load_dataset(\"penguins\")\n",
    "penguins = penguins.dropna()\n",
    "\n",
    "# Separate features and target\n",
    "X = penguins.drop(columns=['species'])\n",
    "y = penguins['species']\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = X.select_dtypes(include='object').columns\n",
    "encoder = OrdinalEncoder()\n",
    "X[categorical_cols] = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rfc = RandomForestClassifier(max_features=X_train.shape[1], random_state=0)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = rfc.predict(X_test)\n",
    "print(f\"Number of trees in the forest: {len(rfc.estimators_)}\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test set accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Gini-based feature importance (impurity-based)\n",
    "importances = rfc.feature_importances_\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Gini Importance': importances\n",
    "}).sort_values('Gini Importance', ascending=False)\n",
    "print(feature_imp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef549ab-8e50-4790-a9a3-4c291b001f60",
   "metadata": {},
   "source": [
    "Next, calculate PFI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f330fb4-00af-4e1c-82d1-6efd74c7a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# PFI on Test Set\n",
    "pfi_test_result = permutation_importance(\n",
    "    rfc, X_test, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "pfi_test_df = pd.DataFrame({\n",
    "    'Feature': X_test.columns,\n",
    "    'Permutation Importance (Test)': pfi_test_result.importances_mean\n",
    "}).sort_values('Permutation Importance (Test)', ascending=False)\n",
    "\n",
    "# PFI on Training Set\n",
    "pfi_train_result = permutation_importance(\n",
    "    rfc, X_train, y_train,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "pfi_train_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Permutation Importance (Train)': pfi_train_result.importances_mean\n",
    "}).sort_values('Permutation Importance (Train)', ascending=False)\n",
    "\n",
    "# Merge for side-by-side comparison\n",
    "pfi_combined_df = pd.merge(\n",
    "    pfi_train_df,\n",
    "    pfi_test_df,\n",
    "    on='Feature'\n",
    ").set_index('Feature')\n",
    "\n",
    "print(pfi_combined_df)\n",
    "\n",
    "# Plot both side-by-side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "# Train PFI\n",
    "pfi_combined_df.sort_values(\"Permutation Importance (Train)\", ascending=True)['Permutation Importance (Train)'].plot.barh(ax=ax[0], color=\"salmon\")\n",
    "ax[0].set_title(\"Permutation Importance (Train Set)\")\n",
    "\n",
    "# Test PFI\n",
    "pfi_combined_df.sort_values(\"Permutation Importance (Test)\", ascending=True)['Permutation Importance (Test)'].plot.barh(ax=ax[1], color=\"skyblue\")\n",
    "ax[1].set_title(\"Permutation Importance (Test Set)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee3e58-fe05-4520-a9e6-a9d8f69ee264",
   "metadata": {},
   "source": [
    "- \"bill_length_mm\" is clearly the most important feature, also according to Gini importance.\n",
    "- \"flipper_length_mm\" and \"bill_depth_mm\" appear overestimated in Gini importance when comparing it to PFI (for both, training and testing sets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aafcce-49ef-4988-b4bf-fa8323900228",
   "metadata": {},
   "source": [
    "## References and Further Learning\n",
    "\n",
    "Apley, D. W. and Zhu, J.: Visualizing the effects of predictor variables in black box supervised learning models, Journal of the Royal Statistical Society Series B: Statistical Methodology, 82, 1059–1086, 2020.\n",
    "\n",
    "Dramsch, J. S.: 70 years of machine learning in geoscience in review, Advances in geophysics, 61, 1–55, 2020.\n",
    "\n",
    "Dwivedi, R., Dave, D., Naik, H., Singhal, S., Omer, R., Patel, P., Qian, B., Wen, Z., Shah, T., and Morgan, G.: Explainable AI (XAI): Core ideas, techniques, and solutions, ACM Computing Surveys, 55, 1–33, doi:10.1145/3561048,    2023.\n",
    "\n",
    "Flora, M. L., Potvin, C. K., McGovern, A., and Handler, S.: A machine learning explainability tutorial for atmospheric sciences, Artificial Intelligence for the Earth Systems, 3, e230018, doi:10.1175/AIES-D-23-0018.1,    2024.\n",
    "\n",
    "Friedman, J. H.: Greedy function approximation: a gradient boosting machine, Annals of statistics, 1189–1232, 2001.\n",
    "\n",
    "Goldstein, A., Kapelner, A., Bleich, J., and Pitkin, E.: Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation, journal of Computational and Graphical Statistics, 24, 44–65, doi:10.1080/10618600.2014.907095,  2015.\n",
    "\n",
    "James, G., Witten, D., Hastie, T., Tibshirani, R., and Taylor, J.: An Introduction to Statistical Learning with Applications in Python, An Introduction to Statistical Learning: with Applications in Python, 1, 2023.\n",
    "\n",
    "Jiang, S., Sweet, L., Blougouras, G., Brenning, A., Li, W., Reichstein, M., Denzler, J., Shangguan, W., Yu, G., Huang, F., and Zscheischler, J.: How Interpretable Machine Learning Can Benefit Process Understanding in the Geosciences, Earth's Future, 12, doi:10.1029/2024EF004540,    2024.\n",
    "\n",
    "Lundberg, S. M. and Lee, S.-I.: A unified approach to interpreting model predictions, Advances in neural information processing systems, 30, 2017.\n",
    "\n",
    "Molnar, C.: Interpretable Machine Learning: A Guide for Making Black Box Models Explainable (3rd ed.). Retrieved from christophm.github.io/interpretable-ml-book/, 2025.\n",
    "\n",
    "Ribeiro, M. T., Singh, S., and Guestrin, C.: \"Why Should I Trust You?\", in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135–1144.\n",
    "\n",
    "Shapley, L. S.: A value for n-person games, in: Contributions to the Theory of Games (AM-28), 307–318."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf862d-85ef-407a-8837-d5f751fd7617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
