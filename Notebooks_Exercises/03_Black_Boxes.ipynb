{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59947908-6eb0-4f53-a6c5-0ec7519f3ca9",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034e7fb-b7a2-4f93-9cc1-37bb706a380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn pandas matplotlib scikit-learn numpy tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a72b47-4637-46df-993c-031cf8c479b0",
   "metadata": {},
   "source": [
    "# Black Box Models\n",
    "\n",
    "Black box models cannot be understood or interpreted by themselves. This notebook briefly introduces some the most famous black box models (including in the geosciences) with the goal of offering just enough background to understand both their predictive power and the challenges they pose for interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff957e32-57bb-4aec-bbbf-92945daa7ebe",
   "metadata": {},
   "source": [
    "## 1. Ensemble Methods\n",
    "\n",
    "Ensemble methods combine multiple simple building blocks to create a stronger, more accurate or more robust model. The simple building blocks are also called \"weak learners\" because they aren't very powerful on their own. But when combined to a \"strong learner\", they can be very powerful. Decision trees (see Notebook 2; Sect. 5) are the most common building blocks in many ensemble methods such as Bagging, Random Forests, and Boosting.\n",
    "\n",
    "Ensemble tree methods improve the main weakness of decision trees: their instability. If you train them on slightly different data, they can give very different results. This is called high variance (see Notebook 1; Sect 3.4).\n",
    "\n",
    "### **Interpretation:**\n",
    "\n",
    "The downside of combining many decision trees in an ensemble is reduced interpretability, as it becomes difficult to visualize the overall model or clearly identify which variables are most influential. While individual trees in the ensemble are hard to interpret, we can still assess **feature importance** post-hoc by measuring how much each variable contributes to reducing error across all trees. For regression tasks, this is typically done using the reduction in residual sum of squares (RSS), and for classification, by evaluating the decrease in Gini index. Features that lead to larger reductions are considered more important.\n",
    "\n",
    "In many tree-based ensemble implementations (like in `scikit-learn` or `XGBoost`), feature importance values are automatically computed and stored as part of the model during training. So in practice, you don’t need to calculate them separately after training. However, we gain interpretability “post-hoc” because these importance values are derived after the model is trained. Feature importance in ensemble methods is therefore an example for a model-specific post-hoc XAI method (Fig. 1).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../Images/XAI_Model_Specific.png\" style=\"width: 400px;\">\n",
    "  <div style=\"font-size: 14px; margin-top: 8px;\">Fig. 1 Overview of interpretability methods in machine learning, modified from Molnar (2025)</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eabd758-8c4e-45ce-b26e-53fa2e9d0c33",
   "metadata": {},
   "source": [
    "## 1.1 Bagging\n",
    "\n",
    "Bagging is using the **bootstrap** method. It makes many new datasets by randomly sampling from the original data (with replacement). To create a bootstrapped dataset that is the same size as the original, we randomly select samples from the original dataset. It is allowed to pick the same sample more than once.\n",
    "\n",
    "With bagging, a separate decision tree is trained on each of these datasets and trees are not pruned (each tree has high variance, but low bias). Then, their predictions are averaged (for numbers) or a majority vote is taken (for categories). This reduces the variance. In this way, hundreds or thousands of trees are being combined into a single procedure. The result is a model that is more stable and accurate than a single tree. Bagging is the basis for methods like **Random Forests**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf6909-2da3-4cae-84d5-0a3776d5d82d",
   "metadata": {},
   "source": [
    "## 1.2 Random Forests\n",
    "\n",
    "Random Forests improve on Bagging by adding a simple but important twist: when a tree decides how to split, it only chooses from a **random subset of features**, rather than all features. This prevents all trees from using the same strong predictors and becoming too similar. By decorrelating the trees, Random Forests reduce overall model variance more effectively than Bagging, leading to better prediction accuracy. In practice, we typically choose the number of features at each split as about the square root of the total number of features. Like Bagging, Random Forests remain stable as more trees are added and do not overfit with large numbers of trees.\n",
    "\n",
    "Fig. 2 illustrates how using a bootstrapped sample and considering only a subset of the variables at each step results in a wide variety of trees (imagine we would have not only 3 trees but hundreds of trees).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../Images/RF.png\" style=\"width: 400px;\">\n",
    "  <div style=\"font-size: 14px; margin-top: 8px;\">Fig. 2 A Random Forest combines predictions from multiple decision trees (DTs), trained on different data subsets, using averaging or majority voting to produce a more robust final result.</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17f47a-3499-4806-b42d-25bc9b44ccde",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's again fit a tree classifier using the Palmer Penguins dataset (compare Notebook 2; Sects. 2 and 5). However, now we use Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c72c3-4f48-4333-8e03-c7cfb96766c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff730e-c261-467d-a818-4ffe27232283",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69010365-9c4a-4559-8cf2-2ccc1201a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset(\"penguins\")\n",
    "penguins = penguins.dropna()\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa97f1-1668-4029-9642-6c2bca47bdf1",
   "metadata": {},
   "source": [
    "Data preparation as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdaa694-518f-4540-973e-6ab11d32e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target:\n",
    "X = penguins.drop(columns=['species'])\n",
    "y = penguins['species']\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Encode categorical features (e.g. OneHotEncoder, OrdinalEncoder): \n",
    "categorical_cols = X.select_dtypes(include='object').columns\n",
    "encoder = OrdinalEncoder()\n",
    "X[categorical_cols] = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Train-test split:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d2fbe2-9a7a-426e-bf29-a723b0dc768d",
   "metadata": {},
   "source": [
    "Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86414f1f-5121-4ec9-b015-9ed69b172d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_features=X_train.shape[1], random_state=0)\n",
    "rfc.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164eb9dc-a361-4dea-9b99-8b4d5ea68fc9",
   "metadata": {},
   "source": [
    "Predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a2303-0d4b-4fe4-9267-a5157557f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8746e9f6-6f47-4c5c-a575-08ea7f88062f",
   "metadata": {},
   "source": [
    "Check how many trees were built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389c046-e805-4c75-9ec3-0cac7cf38210",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of trees in the forest: {len(rfc.estimators_)}\") # n_estimators=100 is the default in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8336b7a-2d34-4af6-b727-ef8015501405",
   "metadata": {},
   "source": [
    "Evaluate with classification metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca285b1-224b-4999-b6b4-d6e36c4118c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test set accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ea809-044b-4cad-b75e-af493d01bd1f",
   "metadata": {},
   "source": [
    "The accuracy is nearly perfect, but this was already achieved with the single decision tree in Notebook 2, as this dataset isn't very challenging. Still, let’s explore how we can inspect feature importance in a Random Forest, since this is one of the options we have to interpret parts of the model. We calculate the build-in feature importance, the Gini importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710eb5a-80d5-422d-9de3-ea8c5ce39019",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rfc.feature_importances_\n",
    "feature_imp_df = pd.DataFrame({'Feature': feature_names, 'Gini Importance': importances}).sort_values('Gini Importance', ascending=False) \n",
    "print(feature_imp_df)\n",
    "\n",
    "# Visualize:\n",
    "sorted_features = feature_imp_df['Feature']\n",
    "sorted_importances = feature_imp_df['Gini Importance']\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.barh(sorted_features, sorted_importances, color='skyblue')\n",
    "plt.xlabel('Gini Importance')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30e684-342e-4425-abf5-29a7519f5b60",
   "metadata": {},
   "source": [
    "## 1.3 Boosting\n",
    "\n",
    "Boosting is an ensemble learning technique that turns a weak learner into a strong one by training models **sequentially**. Unlike Bagging, which builds trees independently using bootstrap samples, Boosting does not rely on random sampling with replacement. Instead, it fits new models to the residual errors of previous ones. Each subsequent model is trained to correct the mistakes of the prior model, gradually improving the overall prediction.\n",
    "\n",
    "In Boosting, instances that are predicted incorrectly are given more importance in the next iteration, often by adjusting their weights or by minimizing a loss function (as in **Gradient Boosting**). A key element of boosting is the learning rate, also known as the shrinkage parameter, which controls how much each new model contributes to the final prediction. This slow, deliberate adjustment process helps reduce overfitting and results in a model that performs well on complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befd520-b465-4b7c-b486-3e0ee5cbb7d9",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 2. k-Nearest Neighbors\n",
    "\n",
    "In contrast to linear regression, k-Nearest Neighbors (KNN) is a non-parametric method for regression and classification tasks that makes no assumptions about the form of the relationship between predictors and the outcome. It does, however, assume that similar data points are located near each other and can be grouped together. With that it can adapt to complex patterns by averaging nearby data points.\n",
    "\n",
    "Given a value for K and a prediction point x₀, **KNN regression** first identifies the **K training points (neighbors)** that are closest to x₀, represented by N₀. It then estimates f(x₀) using the **average** of all the training responses in N₀ (James et al. 2023):\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_0) = \\frac{1}{K} \\sum_{x_i \\in N_0} y_i\n",
    "$$\n",
    "\n",
    "In **KNN classification**, the K nearest neighbors are used to count how many of them belong to each class. The prediction point x₀ is then assigned to the class with the highest frequency among those neighbors.\n",
    "\n",
    "In summary, KNN requires two main components: the number of K and a distance metric to measure similarity between data points. The distance metric can be calculated using various measures such as Euclidian distance. The number of K controls how many neighbors the model uses to make a prediction or classification. A small K (like 1 or 2) allows the model to be very flexible and responsive to the training data (low bias), but it can also make predictions unstable and sensitive to noise (high variance). A larger K averages over more neighbors, leading to smoother and more stable predictions (lower variance), but it may miss important local patterns (higher bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04183b-b2e4-4e50-b3c8-009da61cebef",
   "metadata": {},
   "source": [
    "---\n",
    "To demonstrate KNN we pick up the following example by GeoSMART 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3f2d3-0976-40e0-afa1-0f10f7535f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8fd18-5ca7-442d-b245-00874e8f1d23",
   "metadata": {},
   "source": [
    "We first create the synthetic dataset representing three rock types: Granite, Basalt, and Sandstone. Each type will have characteristic values for density and magnetic susceptibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3ac0d-06c3-48bc-a6ae-499aebb68701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples per class\n",
    "n_samples = 100\n",
    "\n",
    "# Generate features for Granite\n",
    "granite_density = np.random.normal(2.9, 0.2, n_samples)\n",
    "granite_susceptibility = np.random.normal(0.0001, 0.0001, n_samples)\n",
    "granite_label = ['Granite'] * n_samples\n",
    "\n",
    "# Generate features for Basalt\n",
    "basalt_density = np.random.normal(3.2, 0.2, n_samples)\n",
    "basalt_susceptibility = np.random.normal(0.001, 0.0005, n_samples)\n",
    "basalt_label = ['Basalt'] * n_samples\n",
    "\n",
    "# Generate features for Sandstone\n",
    "sandstone_density = np.random.normal(2.4, 0.2, n_samples)\n",
    "sandstone_susceptibility = np.random.normal(0.00005, 0.00005, n_samples)\n",
    "sandstone_label = ['Sandstone'] * n_samples\n",
    "\n",
    "# Combine data\n",
    "density = np.concatenate([granite_density, basalt_density, sandstone_density])\n",
    "susceptibility = np.concatenate([granite_susceptibility, basalt_susceptibility, sandstone_susceptibility])\n",
    "labels = np.concatenate([granite_label, basalt_label, sandstone_label])\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Density': density,\n",
    "    'Magnetic Susceptibility': susceptibility,\n",
    "    'Lithology': labels\n",
    "})\n",
    "data.head()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 3)) \n",
    "sns.scatterplot(\n",
    "    x='Density',\n",
    "    y='Magnetic Susceptibility',\n",
    "    hue='Lithology',\n",
    "    data=data\n",
    ")\n",
    "plt.title('Rock Types Based on Density and Magnetic Susceptibility')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91913fca-a412-46ff-88d7-6ce89208a0eb",
   "metadata": {},
   "source": [
    "Assign X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3e340-ba86-4d0c-a7b4-1bc3b028e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['Density', 'Magnetic Susceptibility']]\n",
    "y = data['Lithology']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad63c5-4642-4166-92e2-3a3b5fd6ad2d",
   "metadata": {},
   "source": [
    "Magnetic susceptibility and density are on very different scales. In KNN, the feature with the larger numeric range (density) would dominate the distance calculation. We therefore scale features first before classifying: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d74f31-ebe1-42a1-bb11-faae12337dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unscaled = data[['Density', 'Magnetic Susceptibility']] # save unscaled data before scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a016443-479d-4f24-bbc0-c86525cb36d1",
   "metadata": {},
   "source": [
    "Split the data between training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e23098-a446-4685-8cab-c172c58a7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a427e2a-a849-4fd4-bd60-a86b6028ec8e",
   "metadata": {},
   "source": [
    "Train a KNN classifier on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1c96f-be37-4b64-a5dc-e81e4359b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0933d7b-db7a-4c7b-a96d-1196f7d0b45c",
   "metadata": {},
   "source": [
    "Evaluate performance using common metrics for classification tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ee7a1-6a3c-4790-b1b2-bae7a174c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0051abf-a359-4676-93d7-54b99dc55a05",
   "metadata": {},
   "source": [
    "### **Interpretation:**\n",
    "\n",
    "By printing the test sample, its nearest neighbors, and their labels we can try to explain the model outputs. Let's check the first test point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2425742-31bb-4d3a-a61e-4316bce5a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X_test[0]\n",
    "distances, indices = classifier.kneighbors([sample])\n",
    "\n",
    "print(\"Test point:\")\n",
    "print(sample)\n",
    "\n",
    "print(\"\\nNearest neighbors (from training set):\")\n",
    "print(X_train[indices[0]])\n",
    "print(\"\\nTheir true labels:\")\n",
    "print(y_train.iloc[indices[0]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503774e4-7d84-4995-9db8-28125ad50374",
   "metadata": {},
   "source": [
    "The test point was classified as Sandstone because all 5 of its nearest neighbors are labeled Sandstone. They are extremely close in both density and magnetic susceptibility.\n",
    "\n",
    "Let's explore this further by visualizing the decision boundaries across the feature space using a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d5024-585b-4a8a-8ae6-24f197640dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision boundary plot\n",
    "h = 0.001\n",
    "x_min, x_max = X_unscaled['Density'].min() - 0.1, X_unscaled['Density'].max() + 0.1\n",
    "y_min, y_max = X_unscaled['Magnetic Susceptibility'].min() - 0.0001, X_unscaled['Magnetic Susceptibility'].max() + 0.0001\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "mesh_points_scaled = scaler.transform(mesh_points)\n",
    "\n",
    "Z = classifier.predict(mesh_points_scaled)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "label_map = {label: i for i, label in enumerate(np.unique(y))}\n",
    "Z_int = np.vectorize(label_map.get)(Z)\n",
    "colors = ['#FFA07A', '#87CEFA', '#90EE90']\n",
    "cmap_light = ListedColormap(colors)\n",
    "\n",
    "plt.figure(figsize=(5, 3)) \n",
    "plt.contourf(xx, yy, Z_int, cmap=cmap_light, alpha=0.5)\n",
    "sns.scatterplot(x='Density', y='Magnetic Susceptibility', hue='Lithology', data=data, edgecolor='k')\n",
    "plt.title(\"KNN Classification Decision Boundary (K = 5)\")\n",
    "plt.xlabel(\"Density\")\n",
    "plt.ylabel(\"Magnetic Susceptibility\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c7b80-545c-4fe3-8802-f03fd9464d08",
   "metadata": {},
   "source": [
    "The colored regions illustrate where each class would be predicted in the feature space. However, KNN is not as globally interpretable as a linear model. It's difficult to explain why a prediction was made beyond saying \"these K points were closest.\" With only two features, this is visually intuitive, and we see that KNN adapts to the local class density. However, in high-dimensional spaces, data points become more isolated, making it harder to identify truly close neighbors. This not only affects predictive performance but also reduces interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074ced0-91e0-455c-9a96-d5f7e2556ac6",
   "metadata": {},
   "source": [
    "> ### **Exercise 1:**\n",
    ">\n",
    ">In some areas of the KNN decision boundary plot, it appears that a point surrounded by visually similar neighbors (e.g. a blue dot among other blue dots) is misclassified. What could explain this apparent misclassification when considering the data preparation steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cb0f4d-567f-4993-a4ef-cfc8eb3784ab",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 3. Support Vector Machines\n",
    "\n",
    "The core idea of Support Vector Machines (SVMs) is to find the optimal **hyperplane** that separates data into classes by maximizing the **margin** - the distance between the hyperplane and the nearest training points from each class, known as **support vectors**. In simple cases where the data can be perfectly divided, this results in a maximal margin classifier, which draws the widest possible boundary between the classes. However, perfect separation is rare in real-world data due to overlaps or outliers. For example, in Fig. 3A, although we can separate the green and blue points by eye, the two groups come very close to each other. This makes the margin very narrow, meaning the boundary would be sensitive to small changes in the data. \n",
    "\n",
    "In these situations, SVMs allow some flexibility: a few points can be on the wrong side of the margin or even misclassified. This more adaptable version is called the **support vector classifier**. Instead of strictly separating all points, it aims to find a boundary that separates most of the data well while still maintaining a reasonably wide margin. Choosing a threshold like this is another example of the bias–variance tradeoff (see Notebook 1; Sect. 3.4): If we won't allow missclassifications, we would pick a threshold that is very sensitive to the training data including outliers but would not perform robust with new data (low bias; high variance). When we allow misclassifications, the distance between the data points and the threshold is called **soft margin** and we calculate the best soft margin (how many misclassifications and data points inside) using cross-validation. For example, Fig. 3B shows a soft margin with one misclassification and two data points that are correctly classified to be within the margin. Fig. 3C shows how the support vector classifier is a line in a 2-dimensional space (with 2 features). In addition, with 3-dimensional data the support vector classifier would form a plane. \n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img src=\"../Images/SVM.png\" alt=\"Regression line and squiggle on training data\" style=\"width: 500px;\">\n",
    "  <figcaption style=\"font-size: 14px; margin-top: 8px;\">\n",
    "    Fig. 3 Support Vector Machines and margin-based classification, modified from \n",
    "    <a href=\"https://www.youtube.com/watch?v=Gv9_4yMHFhI&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF\" target=\"_blank\">\n",
    "      Josh Starmer (YouTube) \n",
    "    </a>\n",
    "      A. One-dimensional data with a narrow margin between two classes (blue and green). B. A soft margin classifier allows for one misclassification (green point on the blue side) and two correctly classified points within the margin zone. C. The support vector classifier in a two-dimensional feature space, with the decision boundary (solid line), margins (dotted lines).\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "SVMs can also handle more complex, non-linear boundaries using what’s known as the **kernel trick**. Rather than drawing a straight line or flat plane, this method transforms the data into a higher-dimensional space where Kernel functions systematically find boundaries to be drawn (support vector classifiers), without having to compute that transformation directly.\n",
    "\n",
    "### **Interpretation:**\n",
    "\n",
    "In simple, low-dimensional settings, it’s often possible to visualize how an SVM separates data and which points (the support vectors) influence the decision boundary - much like how nearest neighbors affect predictions in KNN. However, in higher-dimensional spaces, especially when kernel functions are used, the transformations and resulting decision boundary become much harder to interpret. This is why SVMs are generally considered black box models. The learned decision function is typically complex and often impossible to express in human-readable form."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39b89915-f178-4cd4-a268-6def58ba8cd2",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 4. Neural Networks\n",
    "\n",
    "Neural networks (NNs) form the foundation of many recent advances in AI, thanks to their ability to model complex, high-dimensional data. They are inspired by the way the human brain works: They consist of layers of connected \"neurons\" that process information. We see NNs in everyday life, for example, in voice assistants, image recognition, or large language models (LLMs). Fig. 4A shows a very simple example of a NN. It has an **input layer**, two **hidden layers** (each with five **nodes**), and an **output layer**. Compared to this, LLMs are like super-sized versions with billions of connections. Deep neural networks, commonly referred to as **deep learning** models, are widely used in science for a variety of tasks. Notable examples include convolutional neural networks (CNNs) for image recognition or spatially structured data such as satellite images, recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) for time series and language tasks, and transformer-based models like BERT and GPT for natural language processing.\n",
    "\n",
    "When building a NN, we need to make several design choices, such as:\n",
    "- How many hidden layers to include, and how many nodes to place in each (essentially making an educated guess).\n",
    "- Which loss function to optimize (e.g., mean squared error, or MSE).\n",
    "- Which activation functions to use.\n",
    "- …\n",
    "\n",
    "There are many more adjustments and a variety of algorithms available. However, for now, let’s focus on what all NNs have in common to better understand their basic functionality and their limitations when it comes to interpretability. \n",
    "\n",
    "**All NNs share a basic structure of interconnected layers with weighted connections, activation functions, and an optimization process that adjusts weights based on a loss function during training.** Fig. 4B that shows a tiny NN applied to the regression task from Notebook 2 (find the optimal drug dosage for patients by modeling the relationship between dosage and effectiveness). The NN has a single node in the input layer for the input feature \"Dosage\" and a single node in the output layer for the target feature \"Efficiancy\". We see and know these inputs and outputs. What makes NNs black boxes are their hidden layers. We here have a single hidden layer with two nodes. This is what is generally happening inside:\n",
    "\n",
    "1. The input (dosage) is passed to the two hidden neurons.\n",
    "\n",
    "2. Each hidden neuron is doing:\n",
    "- A weighted sum of inputs (from dosage)\n",
    "- A non-linear transformation (e.g. SoftMax; ReLu)\n",
    "\n",
    "3. The final output is:\n",
    "- A weighted sum of the two activated outputs\n",
    "- Plus a bias term\n",
    "- Sometimes there is an extra activation (not in Fig. 4B)\n",
    "\n",
    "### **Interpretation:**\n",
    "\n",
    "Weights and biases are learned during training: The NN makes a prediction, compares it to the correct answer using a loss function, and then adjusts the weights and biases step by step using an optimization algorithm (like **gradient descent**) to reduce the error. Even though this is a tiny network, it's already difficult to intuitively interpret because the hidden layer uses non-linearities: The activation functions apply non-linear transformations, the network combines these in complex ways, and bias terms shift the resulting curves. In essence, the hidden layer learns internal representations or features from the input that are:\n",
    "\n",
    "- Non-observable (they’re not in the data).\n",
    "- Intermediate (they’re not final predictions).\n",
    "- Learned (they're shaped during training to help the network make better predictions).\n",
    "\n",
    "Although neural networks lack inherent interpretability, certain internal components, such as weights, activations, and gradients, can be analyzed to gain limited insights. Tools like saliency maps, SHAP, LIME, and feature visualization are used to interpret what parts of the model (like weights, activations, or gradients) are doing. These tools typically don’t explain the weights directly, but they reveal how input signals flow through the network (via gradients or activations) or how the network behaves (via output approximations). In short, they make parts of the \"black box\" visible, but not fully understandable in human terms.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img src=\"../Images/NN.png\" alt=\"Regression line and squiggle on training data\" style=\"width: 500px;\">\n",
    "  <figcaption style=\"font-size: 14px; margin-top: 8px;\">\n",
    "    Fig. 4 Structure and inner workings of neural networks., modified from \n",
    "    <a href=\"https://www.youtube.com/watch?v=83LYR-1IcjA&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=10\" target=\"_blank\">\n",
    "      Josh Starmer (YouTube) \n",
    "    </a>\n",
    "      A. A generic deep neural network with an input layer, multiple hidden layers, and an output layer.\n",
    "      B. A simple neural network with one input (\"Dosage\"), one output (\"Efficiency\"), and a hidden layer with two nodes. Each hidden node computes a weighted sum of the input plus a bias, followed by a non-linear activation. The final prediction is a weighted combination of the hidden activations.\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae28260-8ec0-4fda-83cf-69fb23c83370",
   "metadata": {},
   "source": [
    "> ### **Exercise 2:**\n",
    ">\n",
    ">You've already seen a simple NN that takes one input feature (\"Dosage\") and predicts one output (\"Efficiency\") using a single hidden layer with two neurons.\n",
    "> Now imagine you are building a NN to classify iris flowers (dataset introduced in Notebook 2; Sect. 6) into one of three species: Setosa, Versicolor, or Virginica. You will use two input features: Petal Width and Sepal Width.\n",
    "> Based on your understanding, sketch out how this NN could be structured. Think about: How many input and output nodes are needed? What happens in the hidden layer? Where would weights and biases be placed? How many would be involved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d43e7-20af-439f-8cac-d1cac53527b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following is a simple implementation of a NN that trains on a sample dataset and makes predictions using `TensorFlow` and `Keras`, modified from [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/neural-networks-a-beginners-guide/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa06d0-1920-45c8-af73-a0f0a8aed094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa4bc5-3f39-4455-a83b-e5e100c125a0",
   "metadata": {},
   "source": [
    "Create a dataset. Convert the data into a format suitable for training (usually NumPy arrays). Define features (X) and labels (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea8df9-893c-4758-86ce-aed3e2c783b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'feature1': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'feature2': [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "    'label': [0, 0, 1, 1, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['feature1', 'feature2']].values\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21f4f6-5934-4859-a785-2dd486e4895d",
   "metadata": {},
   "source": [
    "To build a NN, first create an instance of a `Sequential` model. Then, add layers to it using `Dense` layers, which define fully connected layers. For each layer, specify the number of neurons and an activation function. The first `Dense` layer also defines the input shape, while subsequent layers form the hidden and output layers.\n",
    "\n",
    "In this example, the model expects two input features (input_dim=2), processes them through a hidden layer with 8 neurons using the ReLU activation, and produces a single output using the sigmoid activation which is suitable for binary classification tasks.\n",
    "\n",
    "In comparison, in NNs for regression tasks, the output layer often uses a linear activation. It then simply returns the raw output of the neuron without applying any transformation. This linearity is important because it allows the model to predict any real-valued number, not just values in a fixed range. While the hidden layers can be highly nonlinear, the final output here needs to remain unconstrained to represent continuous outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa527739-3c0a-4cd3-86cc-2231b8c360ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=2, activation='relu'))  # Hidden layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f4b94-1c92-4bbb-93b2-c12abd7c0497",
   "metadata": {},
   "source": [
    "The following step compiles the model, which means preparing it for training by specifying:\n",
    "- Loss function: 'binary_crossentropy' is used for binary classification problems. It measures how far the predicted probabilities are from the actual class labels (0 or 1).\n",
    "- Optimizer: 'adam' is an efficient algorithm that adjusts the model’s weights to reduce the loss during training.\n",
    "- Metrics: 'accuracy' tracks how often the model predicts the correct class, helping you monitor performance during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b096c5-b255-4043-a11e-30eaf1b115d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d396d8-8286-4f96-8c84-8f772e9b25a6",
   "metadata": {},
   "source": [
    "The training process for a NN is conceptually similar in Python to how other ML models (like Randpom Forests or linear regression) \"fit\" to data.\n",
    "The following code trains the model on the input data (X) and target labels (y) with the following settings:\n",
    "- `epochs=100`: The model will go through the entire training dataset 100 times.\n",
    "- `batch_size=1`: The weights are updated after every single training example. Larger batches = faster training, but may be less accurate or use more memory. It's like learning in small groups instead of one example at a time.\n",
    "- `verbose=1`: Displays training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d9f52-1223-4fcb-9da4-ddc6b1896abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=100, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8a7af-43c2-4cc4-a4e2-4c8c7aa3de99",
   "metadata": {},
   "source": [
    "Make predictions. We provide the model with one input example for testing (new unseen data): a data point that has two feature values (0.2 and 0.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d212cdf-b7dc-4d65-8a37-9740b54f2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array([[0.2, 0.4]])\n",
    "prediction = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec65bc-001a-4be0-b23e-d15fa7e26337",
   "metadata": {},
   "source": [
    "The model outputs a probability, which we convert into a binary class label (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830c945-4471-453b-9d69-06eaf760c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = (prediction > 0.5).astype(int)\n",
    "print(\"Predicted probability:\", prediction)\n",
    "print(\"Predicted label:\", predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e0d74-9e1a-4970-b892-b3a45d971301",
   "metadata": {},
   "source": [
    "## References and Further Learning\n",
    "\n",
    "Denolle, M., Mehra, A., Todoran, S., Cristea, N., Arendt, A., Henderson, S., Sun, Z., Ni, Y., and Kharita, A.: Machine Learning in the Geosciences, **GeoSMART**, University of Washington eScience Institute, available at: https://geo‑smart.github.io/mlgeo-book/about_this_book/about_this_book.html, last access: 30 June **2025**.\n",
    "\n",
    "James, G., Witten, D., Hastie, T., Tibshirani, R., and Taylor, J.: An Introduction to Statistical Learning with Applications in Python, An Introduction to Statistical Learning: with Applications in Python, 1, 2023.\n",
    "\n",
    "Molnar, C.: Interpretable Machine Learning: A Guide for Making Black Box Models Explainable (3rd ed.). Retrieved from christophm.github.io/interpretable-ml-book/, 2025.\n",
    "\n",
    "[StatQuest with Josh Starmer](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw) on YouTube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a75bb1-581b-440f-81dc-a497917c717f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
